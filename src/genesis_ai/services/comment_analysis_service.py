"""
ëŒ“ê¸€ ë¶„ì„ ì„œë¹„ìŠ¤
YouTube ëŒ“ê¸€ì—ì„œ ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
"""

import re
from collections import Counter
from typing import Optional

from ..api import validate_json_output
from ..utils.logger import get_logger, log_step, log_success

logger = get_logger(__name__)


# === ê°ì • ë¶„ì„ í‚¤ì›Œë“œ ===
POSITIVE_KEYWORDS = [
    "ì¢‹ì•„ìš”",
    "ìµœê³ ",
    "ëŒ€ë°•",
    "ì¶”ì²œ",
    "ë§Œì¡±",
    "íš¨ê³¼",
    "ì§±",
    "êµ¿",
    "ê°ì‚¬",
    "ì™„ë²½",
    "í›Œë¥­",
    "ì‚¬ë‘",
    "ìµœì• ",
    "ì¸ìƒí…œ",
    "ê°“",
    "ë¯¸ì³¤",
    "ì©”ì–´",
    "good",
    "great",
    "best",
    "love",
    "amazing",
    "perfect",
    "awesome",
]

NEGATIVE_KEYWORDS = [
    "ë³„ë¡œ",
    "ì‹¤ë§",
    "í™˜ë¶ˆ",
    "ì‚¬ê¸°",
    "í›„íšŒ",
    "ì•ˆë¨",
    "íš¨ê³¼ì—†",
    "ê°€ì§œ",
    "ìµœì•…",
    "ë¹„ì¶”",
    "ì“°ë ˆê¸°",
    "ëˆë‚­ë¹„",
    "ì§œì¦",
    "ë¶ˆë§Œ",
    "êµ¬ë¦¼",
    "bad",
    "worst",
    "hate",
    "terrible",
    "scam",
    "fake",
    "disappointed",
]

PAIN_KEYWORDS = [
    "ê³ ë¯¼",
    "ë¬¸ì œ",
    "ì–´ë ¤",
    "í˜ë“¤",
    "ê·€ì°®",
    "ë¶ˆí¸",
    "ë‹µë‹µ",
    "ìŠ¤íŠ¸ë ˆìŠ¤",
    "ë¬´ì„œ",
    "ê±±ì •",
    "ì§œì¦",
    "ëª»",
    "ì•ˆë˜",
    "ì™œ",
    "ì–´ë–»ê²Œ",
    "ë„ì™€",
]

QUESTION_PATTERNS = [
    r"ì–´ë””ì„œ\s*(?:ì‚¬|êµ¬ë§¤|êµ¬ì…)",
    r"ì–¼ë§ˆ(?:ì˜ˆìš”|ì—ìš”|ì¸ê°€ìš”|ì•¼)",
    r"íš¨ê³¼\s*(?:ìˆ|ì—†|ì¢‹|ì–´ë•Œ)",
    r"ì¶”ì²œ\s*(?:í•´|ì¢€|ë¶€íƒ)",
    r"\?$",  # ë¬¼ìŒí‘œë¡œ ëë‚˜ëŠ” ë¬¸ì¥
]


class CommentAnalysisService:
    """YouTube ëŒ“ê¸€ ë¶„ì„ ì„œë¹„ìŠ¤ (Hybrid: Rule-based + AI)"""

    def __init__(self, gemini_client=None) -> None:
        """
        Args:
            gemini_client: AI ê¸°ë°˜ ì‹¬ì¸µ ë¶„ì„ ì‹œ ì‚¬ìš© (í•„ìˆ˜)
        """
        self._gemini = gemini_client
        self.pipeline: Optional["PipelineOrchestrator"] = None

        # X-Algorithm Pipeline ì´ˆê¸°í™”
        if self._gemini:
            from genesis_ai.services.pipeline import (
                CommentSource,
                EngagementScorer,
                FeatureHydrator,
                PipelineOrchestrator,
                QualityFilter,
                TopInsightSelector,
            )

            self.pipeline = PipelineOrchestrator(
                source=CommentSource(),
                hydrator=FeatureHydrator(gemini_client),  # gemini_client ì¬ì‚¬ìš©
                quality_filter=QualityFilter(),
                scorer=EngagementScorer(),
                selector=TopInsightSelector(),
            )
        else:
            self.pipeline = None

    def analyze_comments(self, comments: list[dict]) -> dict:
        """
        ëŒ“ê¸€ ê¸°ë³¸ ë¶„ì„ (Rule-based Fast Analysis)
        """
        log_step("ëŒ“ê¸€ ê¸°ë³¸ ë¶„ì„", "ì‹œì‘", f"{len(comments)}ê°œ ëŒ“ê¸€")

        if not comments:
            return self._empty_result()

        # ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = [c.get("text", "") for c in comments if c.get("text")]

        # ê° ë¶„ì„ ìˆ˜í–‰
        sentiment = self._analyze_sentiment(texts)
        pain_points = self._extract_pain_points(texts)
        gain_points = self._extract_gain_points(texts)
        questions = self._extract_questions(texts)
        keywords = self._extract_keywords(texts)

        result = {
            "total_comments": len(comments),
            "sentiment": sentiment,
            "pain_points": pain_points,
            "gain_points": gain_points,
            "questions": questions,
            "top_keywords": keywords,
            "summary": self._generate_summary(sentiment, pain_points, gain_points),
            "ai_analysis": None,  # AI ë¶„ì„ ê²°ê³¼ ê³µê°„ í™•ë³´
        }

        log_success(
            f"ëŒ“ê¸€ ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ: ê¸ì • {sentiment['positive']}%, ë¶€ì • {sentiment['negative']}%"
        )
        return result

    def analyze_with_ai(self, comments: list[dict]) -> dict:
        """
        AIë¥¼ í™œìš©í•œ ì‹¬ì¸µ ëŒ“ê¸€ ë¶„ì„ (Deep Analysis)
        - Rule-based ë¶„ì„ ê²°ê³¼ì— AI ì¸ì‚¬ì´íŠ¸ë¥¼ í†µí•©í•©ë‹ˆë‹¤.
        """
        # 1. ê¸°ë³¸ ë¶„ì„ ë¨¼ì € ìˆ˜í–‰
        base_result = self.analyze_comments(comments)

        if not self._gemini or not comments:
            return base_result

        # [NEW] X-Algorithm Pipeline ì‹¤í–‰
        if self.pipeline:
            try:
                import asyncio

                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
                try:
                    loop = asyncio.get_running_loop()
                except RuntimeError:
                    loop = None

                if loop and loop.is_running():
                    # ì´ë¯¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ (ì˜ˆ: Streamlitì´ ë¹„ë™ê¸°ë¡œ ë„ëŠ” ê²½ìš°) create_task ë“±ì„ ì¨ì•¼ í•˜ëŠ”ë°,
                    # ë™ê¸° í•¨ìˆ˜ ë‚´ë¼ì„œ awaitë¥¼ ëª» ì”€.
                    # ì´ ê²½ìš°ì—” nest_asyncio ê°™ì€ ê²Œ í•„ìš”í•œë°, ì¼ë‹¨ ê°„ë‹¨íˆ run_pipelineì´ asyncì´ë¯€ë¡œ
                    # ë™ê¸° ë˜í¼ê°€ í•„ìš”í•¨.
                    # ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ ë³„ë„ ìŠ¤ë ˆë“œë‚˜ nest_asyncioë¥¼ ê°€ì •í•˜ì§€ ì•Šê³ ,
                    # ë§Œì•½ ë£¨í”„ê°€ ì—†ë‹¤ë©´ run()ì„, ìˆë‹¤ë©´... ë³µì¡í•¨.
                    # ê°„ë‹¨í•˜ê²Œ: Streamlitì€ ë³´í†µ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ëŒê³  ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš°ê°€ ë§ìŒ.
                    pipeline_result = self._run_async(self.pipeline.run_pipeline(comments))
                else:
                    pipeline_result = self._run_async(self.pipeline.run_pipeline(comments))

                if pipeline_result and "insights" in pipeline_result:
                    base_result["x_algorithm_insights"] = pipeline_result["insights"]
                    base_result["x_algorithm_stats"] = pipeline_result["stats"]
                    log_success(
                        f"X-Algorithm Pipeline ì™„ë£Œ: {len(pipeline_result['insights'])}ê°œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ"
                    )

            except Exception as e:
                logger.error(f"X-Algorithm Pipeline Error: {e}")

        log_step("AI ì‹¬ì¸µ ë¶„ì„", "Gemini Pro", "ê³ ê° ë‹ˆì¦ˆ/í˜ì¸í¬ì¸íŠ¸ ì¶”ì¶œ ì¤‘...")

        # 2. ë¶„ì„ìš© í…ìŠ¤íŠ¸ ì¤€ë¹„ (ìƒìœ„ 50~100ê°œ ëŒ“ê¸€ w/ filtering)
        # ë„ˆë¬´ ì§§ì€ ëŒ“ê¸€(3ê¸€ì ë¯¸ë§Œ) ì œì™¸
        valid_comments = [
            c.get("text", "") for c in comments if len(c.get("text", "")) > 3
        ]
        sample_texts = valid_comments[:70]  # ë¹„ìš© ìµœì í™”ë¥¼ ìœ„í•´ ìƒìœ„ 70ê°œë§Œ ë¶„ì„
        combined_text = "\n- ".join(sample_texts)

        # 3. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ (Insight Extraction)
        prompt = f"""
ë‹¤ìŒì€ ì œí’ˆ/ì„œë¹„ìŠ¤ ì˜ìƒì— ë‹¬ë¦° YouTube ì‹œì²­ì ëŒ“ê¸€ë“¤ì…ë‹ˆë‹¤.
ë§ˆì¼€í„°ì˜ ê´€ì ì—ì„œ ì´ ëŒ“ê¸€ë“¤ì„ ì‹¬ì¸µ ë¶„ì„í•˜ì—¬ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ì£¼ì„¸ìš”.

### ğŸ“ ë¶„ì„ ëŒ€ìƒ ëŒ“ê¸€ (ìƒ˜í”Œ)
{combined_text}

### ğŸ•µï¸â€â™‚ï¸ ë¶„ì„ ìš”ì²­ ì‚¬í•­
ë‹¨ìˆœí•œ ìš”ì•½ì´ ì•„ë‹ˆë¼, **'íŒë§¤ ì „í™˜'**ì— ë„ì›€ì´ ë˜ëŠ” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
ë‹¤ìŒ JSON í¬ë§·ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

{{
    "customer_sentiment": {{
        "dominant_emotion": "ì§€ë°°ì ì¸ ê°ì • (ì˜ˆ: ê¸°ëŒ€ê°, ì‹¤ë§, í˜¸ê¸°ì‹¬)",
        "sentiment_reason": "ìœ„ ê°ì •ì´ ë‚˜íƒ€ë‚˜ëŠ” ì£¼ëœ ì´ìœ "
    }},
    "deep_pain_points": [
        "ê³ ê°ì´ í˜¸ì†Œí•˜ëŠ” êµ¬ì²´ì ì¸ ë¬¸ì œì /ë¶ˆí¸í•¨ 1",
        "ê³ ê°ì´ í˜¸ì†Œí•˜ëŠ” êµ¬ì²´ì ì¸ ë¬¸ì œì /ë¶ˆí¸í•¨ 2",
        "ê³ ê°ì´ í˜¸ì†Œí•˜ëŠ” êµ¬ì²´ì ì¸ ë¬¸ì œì /ë¶ˆí¸í•¨ 3"
    ],
    "buying_factors": [
        "ê³ ê°ì´ ì œí’ˆì„ êµ¬ë§¤í•˜ê³  ì‹¶ì–´í•˜ëŠ” í•µì‹¬ ì´ìœ  1",
        "ê³ ê°ì´ ì œí’ˆì„ êµ¬ë§¤í•˜ê³  ì‹¶ì–´í•˜ëŠ” í•µì‹¬ ì´ìœ  2"
    ],
    "marketing_hooks": [
        "ëŒ“ê¸€ì˜ ëª©ì†Œë¦¬ë¥¼ ë°˜ì˜í•œ ê´‘ê³  ì¹´í”¼ 1",
        "ëŒ“ê¸€ì˜ ëª©ì†Œë¦¬ë¥¼ ë°˜ì˜í•œ ê´‘ê³  ì¹´í”¼ 2",
        "ëŒ“ê¸€ì˜ ëª©ì†Œë¦¬ë¥¼ ë°˜ì˜í•œ ê´‘ê³  ì¹´í”¼ 3"
    ],
    "faq_candidates": [
        "ìì£¼ ë¬»ëŠ” ì§ˆë¬¸/ì˜¤í•´ 1",
        "ìì£¼ ë¬»ëŠ” ì§ˆë¬¸/ì˜¤í•´ 2"
    ],
    "executive_summary": "ì „ì²´ ë¶„ì„ ê²°ê³¼ë¥¼ 3ë¬¸ì¥ ë‚´ì™¸ë¡œ ìš”ì•½ (ë§ˆì¼€í„° ë³´ê³ ìš©)"
}}

ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""

        try:
            # 4. Gemini í˜¸ì¶œ (Retry Logic í¬í•¨)
            response_text = self._gemini.generate_text(prompt, temperature=0.4)

            # 5. ê²°ê³¼ ê²€ì¦ ë° ì •í™”
            ai_data = validate_json_output(
                response_text, required_fields=["deep_pain_points", "marketing_hooks"]
            )

            # 6. ê²°ê³¼ í†µí•©
            base_result["ai_analysis"] = ai_data

            # AI ìš”ì•½ì´ ìˆë‹¤ë©´ ìµœìƒìœ„ ìš”ì•½ ë®ì–´ì“°ê¸° (ë” ì •í™•í•˜ë¯€ë¡œ)
            if "executive_summary" in ai_data:
                base_result["summary"] = f"[AI] {ai_data['executive_summary']}"

            log_success("AI ì‹¬ì¸µ ë¶„ì„ ì™„ë£Œ")
            return base_result

        except Exception as e:
            logger.error(f"AI ëŒ“ê¸€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜ (ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ë°©ì§€)
            base_result["ai_analysis"] = {"error": str(e)}
            return base_result

    def _run_async(self, coro):
        import asyncio
        from concurrent.futures import ThreadPoolExecutor

        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = None

        if loop and loop.is_running():
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(asyncio.run, coro)
                return future.result()

        return asyncio.run(coro)

    def _empty_result(self) -> dict:
        """ë¹ˆ ê²°ê³¼ ë°˜í™˜"""
        return {
            "total_comments": 0,
            "sentiment": {"positive": 0, "negative": 0, "neutral": 100},
            "pain_points": [],
            "gain_points": [],
            "questions": [],
            "top_keywords": [],
            "summary": "ë¶„ì„í•  ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.",
            "ai_analysis": None,
        }

    def _analyze_sentiment(self, texts: list[str]) -> dict:
        """ê°ì • ë¶„ì„ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë¹„ìœ¨)"""
        positive_count = 0
        negative_count = 0

        for text in texts:
            text_lower = text.lower()

            has_positive = any(kw in text_lower for kw in POSITIVE_KEYWORDS)
            has_negative = any(kw in text_lower for kw in NEGATIVE_KEYWORDS)

            if has_positive and not has_negative:
                positive_count += 1
            elif has_negative and not has_positive:
                negative_count += 1

        total = len(texts) if texts else 1
        neutral_count = total - positive_count - negative_count

        return {
            "positive": round(positive_count / total * 100, 1),
            "negative": round(negative_count / total * 100, 1),
            "neutral": round(neutral_count / total * 100, 1),
            "positive_count": positive_count,
            "negative_count": negative_count,
            "neutral_count": neutral_count,
        }

    def _extract_pain_points(self, texts: list[str], max_count: int = 5) -> list[str]:
        """í˜ì¸í¬ì¸íŠ¸ ì¶”ì¶œ"""
        pain_comments = []

        for text in texts:
            # í˜ì¸ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ëŒ“ê¸€ ìˆ˜ì§‘
            if any(kw in text for kw in PAIN_KEYWORDS):
                # ë„ˆë¬´ ì§§ì€ ëŒ“ê¸€ ì œì™¸
                if len(text) > 10:
                    pain_comments.append(text[:100])  # 100ìë¡œ ì œí•œ

        # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ Nê°œ ë°˜í™˜
        unique_pains = list(set(pain_comments))
        return unique_pains[:max_count]

    def _extract_gain_points(self, texts: list[str], max_count: int = 5) -> list[str]:
        """ê¸ì •ì  í”¼ë“œë°±(Gain Points) ì¶”ì¶œ"""
        gain_comments = []

        for text in texts:
            # ê¸ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ ëŒ“ê¸€ ìˆ˜ì§‘
            if any(kw in text.lower() for kw in POSITIVE_KEYWORDS):
                if len(text) > 10:
                    gain_comments.append(text[:100])

        unique_gains = list(set(gain_comments))
        return unique_gains[:max_count]

    def _extract_questions(self, texts: list[str], max_count: int = 5) -> list[str]:
        """ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ ì¶”ì¶œ"""
        questions = []

        for text in texts:
            # ì§ˆë¬¸ íŒ¨í„´ ë§¤ì¹­
            for pattern in QUESTION_PATTERNS:
                if re.search(pattern, text):
                    if len(text) > 5 and len(text) < 200:
                        questions.append(text)
                    break

        unique_questions = list(set(questions))
        return unique_questions[:max_count]

    def _extract_keywords(self, texts: list[str], max_count: int = 10) -> list[dict]:
        """í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
        all_text = " ".join(texts)

        # ê°„ë‹¨í•œ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (2ê¸€ì ì´ìƒ)
        words = re.findall(r"[ê°€-í£a-zA-Z]{2,}", all_text)

        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            "ê·¸ë¦¬ê³ ",
            "í•˜ì§€ë§Œ",
            "ê·¸ë˜ì„œ",
            "ê·¼ë°",
            "ì´ê±°",
            "ì €ê±°",
            "the",
            "and",
            "is",
            "it",
        }
        filtered_words = [w for w in words if w.lower() not in stopwords]

        # ë¹ˆë„ ê³„ì‚°
        word_counts = Counter(filtered_words)
        top_words = word_counts.most_common(max_count)

        return [{"word": word, "count": count} for word, count in top_words]

    def _generate_summary(
        self,
        sentiment: dict,
        pain_points: list[str],
        gain_points: list[str],
    ) -> str:
        """ë¶„ì„ ìš”ì•½ ìƒì„±"""
        positive_pct = sentiment["positive"]
        negative_pct = sentiment["negative"]

        # ê°ì • ìš”ì•½
        if positive_pct > 60:
            sentiment_summary = "ì „ë°˜ì ìœ¼ë¡œ ë§¤ìš° ê¸ì •ì ì¸ ë°˜ì‘"
        elif positive_pct > 40:
            sentiment_summary = "ê¸ì •ì  ë°˜ì‘ì´ ìš°ì„¸"
        elif negative_pct > 40:
            sentiment_summary = "ë¶€ì •ì  ë°˜ì‘ì— ì£¼ì˜ í•„ìš”"
        else:
            sentiment_summary = "ì¤‘ë¦½ì ì¸ ë°˜ì‘ì´ ëŒ€ë‹¤ìˆ˜"

        # í˜ì¸í¬ì¸íŠ¸ ìš”ì•½
        pain_summary = ""
        if pain_points:
            pain_summary = f" ì£¼ìš” ê³ ë¯¼: {len(pain_points)}ê°œ ë°œê²¬."

        return f"{sentiment_summary}.{pain_summary}"

    def get_marketing_phrases(self, comments: list[dict]) -> list[str]:
        """
        ë§ˆì¼€íŒ…ì— í™œìš©í•  ìˆ˜ ìˆëŠ” ê³ ê° ì–¸ì–´ ì¶”ì¶œ

        Args:
            comments: YouTube ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë§ˆì¼€íŒ… ë¬¸êµ¬ë¡œ í™œìš© ê°€ëŠ¥í•œ í‘œí˜„ë“¤
        """
        phrases = []

        for comment in comments:
            text = comment.get("text", "")
            likes = comment.get("likes", 0)

            # ì¢‹ì•„ìš”ê°€ ë§ì€ ê¸ì •ì  ëŒ“ê¸€ì—ì„œ ë¬¸êµ¬ ì¶”ì¶œ
            if likes >= 5 and any(kw in text.lower() for kw in POSITIVE_KEYWORDS):
                # ì¸ìš©í•  ë§Œí•œ ì§§ì€ ë¬¸ì¥ ì¶”ì¶œ
                sentences = re.split(r"[.!?]", text)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if 10 < len(sentence) < 50:
                        phrases.append(f'"{sentence}"')

        return phrases[:10]  # ìƒìœ„ 10ê°œ
